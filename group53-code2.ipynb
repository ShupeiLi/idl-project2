{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1614d13",
   "metadata": {},
   "source": [
    "# Assignment 2: Building MLPs, CNNs, and Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fa644",
   "metadata": {},
   "source": [
    "## Task 1: Learn the basics of Keras API for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006843a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92b98f73",
   "metadata": {},
   "source": [
    "## Task 2: Develop a \"Tell-the-time\" network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2755afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19402e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47eb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./a2_data/images.npy\")\n",
    "label = np.load(\"./a2_data/labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf715a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data/255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da8c83",
   "metadata": {},
   "source": [
    "Regression Task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435c5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_r=label[:,0]+label[:,1]/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88be9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_sense_reg(y_true, y_pred):\n",
    "    d = K.abs(y_true-y_pred)\n",
    "    return (1-(d//6))*(d)+(d//6)*(12-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bb78af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (14400, 150, 150)\n",
      "Test set: (3600, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "X, X_test, y, y_test = train_test_split(data, label_r, test_size=0.2, random_state=42)\n",
    "print(f\"Train set: {X.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a7c68db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 148, 148, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 74, 74, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 36, 36, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 18, 18, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 8, 8, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,929\n",
      "Trainable params: 72,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regression_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(150,150,1)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",strides=1),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\",strides=2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\",strides=1),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64,activation=\"relu\"),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "830ef99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 60\n",
    "regression_model.compile(loss=\"mse\", optimizer=\"adam\", metrics=common_sense_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28a0ef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "203/203 [==============================] - 5s 19ms/step - loss: 13.7863 - common_sense_reg: 3.0049 - val_loss: 12.3277 - val_common_sense_reg: 3.0180\n",
      "Epoch 2/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 12.1149 - common_sense_reg: 2.9987 - val_loss: 11.9883 - val_common_sense_reg: 2.9832\n",
      "Epoch 3/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 11.7883 - common_sense_reg: 2.9071 - val_loss: 10.9861 - val_common_sense_reg: 2.7747\n",
      "Epoch 4/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 10.5699 - common_sense_reg: 2.6683 - val_loss: 10.0905 - val_common_sense_reg: 2.5747\n",
      "Epoch 5/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 9.7920 - common_sense_reg: 2.4883 - val_loss: 9.7146 - val_common_sense_reg: 2.4516\n",
      "Epoch 6/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 9.4181 - common_sense_reg: 2.4291 - val_loss: 9.0243 - val_common_sense_reg: 2.3464\n",
      "Epoch 7/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 9.0144 - common_sense_reg: 2.3614 - val_loss: 9.6056 - val_common_sense_reg: 2.3529\n",
      "Epoch 8/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 8.7500 - common_sense_reg: 2.3121 - val_loss: 8.4847 - val_common_sense_reg: 2.2380\n",
      "Epoch 9/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 8.5698 - common_sense_reg: 2.2728 - val_loss: 8.4630 - val_common_sense_reg: 2.2552\n",
      "Epoch 10/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 8.0889 - common_sense_reg: 2.2088 - val_loss: 8.2268 - val_common_sense_reg: 2.1907\n",
      "Epoch 11/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 7.9243 - common_sense_reg: 2.1727 - val_loss: 8.0989 - val_common_sense_reg: 2.1804\n",
      "Epoch 12/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 7.4923 - common_sense_reg: 2.1144 - val_loss: 7.5364 - val_common_sense_reg: 2.0734\n",
      "Epoch 13/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 7.1705 - common_sense_reg: 2.0634 - val_loss: 7.4696 - val_common_sense_reg: 2.0487\n",
      "Epoch 14/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 7.0835 - common_sense_reg: 2.0419 - val_loss: 7.3953 - val_common_sense_reg: 2.0650\n",
      "Epoch 15/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 6.7778 - common_sense_reg: 1.9916 - val_loss: 7.5930 - val_common_sense_reg: 1.9991\n",
      "Epoch 16/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 6.5732 - common_sense_reg: 1.9511 - val_loss: 6.8020 - val_common_sense_reg: 1.9470\n",
      "Epoch 17/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 6.4959 - common_sense_reg: 1.9397 - val_loss: 6.9262 - val_common_sense_reg: 1.9954\n",
      "Epoch 18/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 6.2614 - common_sense_reg: 1.9054 - val_loss: 6.7320 - val_common_sense_reg: 1.9413\n",
      "Epoch 19/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 6.1197 - common_sense_reg: 1.8743 - val_loss: 6.4548 - val_common_sense_reg: 1.8784\n",
      "Epoch 20/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 5.9078 - common_sense_reg: 1.8363 - val_loss: 6.6098 - val_common_sense_reg: 1.9125\n",
      "Epoch 21/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 5.8219 - common_sense_reg: 1.8244 - val_loss: 6.5772 - val_common_sense_reg: 1.9010\n",
      "Epoch 22/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 5.6321 - common_sense_reg: 1.7848 - val_loss: 6.2357 - val_common_sense_reg: 1.8392\n",
      "Epoch 23/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 5.5562 - common_sense_reg: 1.7721 - val_loss: 6.1607 - val_common_sense_reg: 1.8290\n",
      "Epoch 24/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 5.3090 - common_sense_reg: 1.7265 - val_loss: 5.7778 - val_common_sense_reg: 1.7757\n",
      "Epoch 25/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 5.2219 - common_sense_reg: 1.7073 - val_loss: 5.9621 - val_common_sense_reg: 1.7666\n",
      "Epoch 26/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 5.0756 - common_sense_reg: 1.6811 - val_loss: 5.7179 - val_common_sense_reg: 1.7640\n",
      "Epoch 27/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 4.9130 - common_sense_reg: 1.6443 - val_loss: 5.5041 - val_common_sense_reg: 1.7239\n",
      "Epoch 28/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 4.7759 - common_sense_reg: 1.6271 - val_loss: 5.7314 - val_common_sense_reg: 1.7856\n",
      "Epoch 29/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 4.5846 - common_sense_reg: 1.5884 - val_loss: 5.3670 - val_common_sense_reg: 1.6894\n",
      "Epoch 30/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 4.6013 - common_sense_reg: 1.6013 - val_loss: 5.3817 - val_common_sense_reg: 1.7087\n",
      "Epoch 31/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 4.2795 - common_sense_reg: 1.5291 - val_loss: 5.0083 - val_common_sense_reg: 1.6231\n",
      "Epoch 32/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 4.1870 - common_sense_reg: 1.5129 - val_loss: 5.1330 - val_common_sense_reg: 1.6849\n",
      "Epoch 33/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 4.0032 - common_sense_reg: 1.4755 - val_loss: 4.7737 - val_common_sense_reg: 1.5710\n",
      "Epoch 34/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 3.8810 - common_sense_reg: 1.4503 - val_loss: 4.7189 - val_common_sense_reg: 1.5583\n",
      "Epoch 35/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 3.7998 - common_sense_reg: 1.4379 - val_loss: 4.7455 - val_common_sense_reg: 1.5844\n",
      "Epoch 36/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 3.7042 - common_sense_reg: 1.4191 - val_loss: 4.5748 - val_common_sense_reg: 1.5480\n",
      "Epoch 37/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 3.5263 - common_sense_reg: 1.3848 - val_loss: 4.4443 - val_common_sense_reg: 1.5284\n",
      "Epoch 38/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 3.4078 - common_sense_reg: 1.3640 - val_loss: 4.4775 - val_common_sense_reg: 1.5379\n",
      "Epoch 39/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 3.2634 - common_sense_reg: 1.3358 - val_loss: 4.4530 - val_common_sense_reg: 1.5223\n",
      "Epoch 40/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 3.2001 - common_sense_reg: 1.3229 - val_loss: 4.3054 - val_common_sense_reg: 1.4991\n",
      "Epoch 41/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 3.0849 - common_sense_reg: 1.3021 - val_loss: 4.2173 - val_common_sense_reg: 1.4754\n",
      "Epoch 42/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 3.0017 - common_sense_reg: 1.2842 - val_loss: 4.4353 - val_common_sense_reg: 1.5208\n",
      "Epoch 43/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.9005 - common_sense_reg: 1.2599 - val_loss: 4.3019 - val_common_sense_reg: 1.4990\n",
      "Epoch 44/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.8110 - common_sense_reg: 1.2444 - val_loss: 3.9225 - val_common_sense_reg: 1.4243\n",
      "Epoch 45/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.6986 - common_sense_reg: 1.2196 - val_loss: 3.9701 - val_common_sense_reg: 1.4359\n",
      "Epoch 46/60\n",
      "203/203 [==============================] - 3s 17ms/step - loss: 2.7321 - common_sense_reg: 1.2305 - val_loss: 4.3264 - val_common_sense_reg: 1.4446\n",
      "Epoch 47/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.5895 - common_sense_reg: 1.1927 - val_loss: 3.8724 - val_common_sense_reg: 1.4190\n",
      "Epoch 48/60\n",
      "203/203 [==============================] - 3s 17ms/step - loss: 2.5227 - common_sense_reg: 1.1853 - val_loss: 4.1372 - val_common_sense_reg: 1.4520\n",
      "Epoch 49/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.5726 - common_sense_reg: 1.1973 - val_loss: 4.3031 - val_common_sense_reg: 1.4915\n",
      "Epoch 50/60\n",
      "203/203 [==============================] - 3s 17ms/step - loss: 2.4372 - common_sense_reg: 1.1658 - val_loss: 3.8232 - val_common_sense_reg: 1.3944\n",
      "Epoch 51/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.3150 - common_sense_reg: 1.1316 - val_loss: 3.9273 - val_common_sense_reg: 1.4203\n",
      "Epoch 52/60\n",
      "203/203 [==============================] - 3s 17ms/step - loss: 2.3149 - common_sense_reg: 1.1374 - val_loss: 3.8043 - val_common_sense_reg: 1.3862\n",
      "Epoch 53/60\n",
      "203/203 [==============================] - 3s 17ms/step - loss: 2.2854 - common_sense_reg: 1.1304 - val_loss: 3.7854 - val_common_sense_reg: 1.3870\n",
      "Epoch 54/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.1554 - common_sense_reg: 1.0917 - val_loss: 3.7583 - val_common_sense_reg: 1.3787\n",
      "Epoch 55/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.1889 - common_sense_reg: 1.1047 - val_loss: 3.7220 - val_common_sense_reg: 1.3808\n",
      "Epoch 56/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.0805 - common_sense_reg: 1.0757 - val_loss: 4.0564 - val_common_sense_reg: 1.4118\n",
      "Epoch 57/60\n",
      "203/203 [==============================] - 4s 17ms/step - loss: 2.0102 - common_sense_reg: 1.0596 - val_loss: 3.7741 - val_common_sense_reg: 1.3983\n",
      "Epoch 58/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 2.0394 - common_sense_reg: 1.0723 - val_loss: 3.7180 - val_common_sense_reg: 1.3774\n",
      "Epoch 59/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 1.9311 - common_sense_reg: 1.0445 - val_loss: 3.7679 - val_common_sense_reg: 1.4081\n",
      "Epoch 60/60\n",
      "203/203 [==============================] - 4s 18ms/step - loss: 1.9467 - common_sense_reg: 1.0470 - val_loss: 3.9400 - val_common_sense_reg: 1.4054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18256585b48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model.fit(X,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40b8237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 4ms/step - loss: 4.0751 - common_sense_reg: 1.4781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.0750956535339355, 1.4780513048171997]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f401f7",
   "metadata": {},
   "source": [
    "Classfication Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fda48",
   "metadata": {},
   "source": [
    "24 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf7d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "972e62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c=[]\n",
    "for i in range(24):\n",
    "    for j in range(750):\n",
    "        y_c.append(i)\n",
    "y_c=np.stack(y_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5215f797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 23, 23, 23])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86ffe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_sense_cla(y_true, y_pred):\n",
    "    y_true = y_true.numpy()\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    d = np.abs(y_true-y_pred)\n",
    "    return 0.5*((1-(d//12))*(d)+(d//12)*(24-d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37e72d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (14400, 150, 150)\n",
      "Test set: (3600, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "X, X_test, y, y_test = train_test_split(data, y_c, test_size=0.2, random_state=42)\n",
    "print(f\"Train set: {X.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148b9d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 16)      160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 36, 36, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 18, 18, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                1560      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 287,064\n",
      "Trainable params: 287,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(150,150,1)),\n",
    "        layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\",strides=1),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",strides=2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\",strides=1),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64,activation=\"relu\"),\n",
    "        layers.Dense(24,activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2920fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "classification_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",common_sense_cla], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b146d1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 6s 29ms/step - loss: 1.7206 - accuracy: 0.3914 - common_sense_cla: 2.9691 - val_loss: 1.8018 - val_accuracy: 0.3708 - val_common_sense_cla: 2.9729\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 1.6363 - accuracy: 0.4153 - common_sense_cla: 2.9675 - val_loss: 1.7506 - val_accuracy: 0.3708 - val_common_sense_cla: 2.9690\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 1.5580 - accuracy: 0.4411 - common_sense_cla: 2.9656 - val_loss: 1.6883 - val_accuracy: 0.3924 - val_common_sense_cla: 2.9714\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 1.4785 - accuracy: 0.4680 - common_sense_cla: 2.9646 - val_loss: 1.6291 - val_accuracy: 0.4222 - val_common_sense_cla: 2.9702\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 1.4146 - accuracy: 0.4849 - common_sense_cla: 2.9686 - val_loss: 1.5767 - val_accuracy: 0.4292 - val_common_sense_cla: 2.9685\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 1.3398 - accuracy: 0.5127 - common_sense_cla: 2.9649 - val_loss: 1.5404 - val_accuracy: 0.4236 - val_common_sense_cla: 2.9694\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 1.3079 - accuracy: 0.5204 - common_sense_cla: 2.9610 - val_loss: 1.5582 - val_accuracy: 0.4396 - val_common_sense_cla: 2.9650\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 1.2356 - accuracy: 0.5465 - common_sense_cla: 2.9582 - val_loss: 1.4998 - val_accuracy: 0.4500 - val_common_sense_cla: 2.9752\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 1.1777 - accuracy: 0.5654 - common_sense_cla: 2.9583 - val_loss: 1.4386 - val_accuracy: 0.4778 - val_common_sense_cla: 2.9695\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 1.1339 - accuracy: 0.5818 - common_sense_cla: 2.9624 - val_loss: 1.4497 - val_accuracy: 0.4722 - val_common_sense_cla: 2.9660\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 1.0977 - accuracy: 0.5940 - common_sense_cla: 2.9627 - val_loss: 1.4323 - val_accuracy: 0.5007 - val_common_sense_cla: 2.9663\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 1.0455 - accuracy: 0.6090 - common_sense_cla: 2.9615 - val_loss: 1.4310 - val_accuracy: 0.5021 - val_common_sense_cla: 2.9707\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 1.0095 - accuracy: 0.6185 - common_sense_cla: 2.9604 - val_loss: 1.3848 - val_accuracy: 0.5208 - val_common_sense_cla: 2.9676\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.9602 - accuracy: 0.6430 - common_sense_cla: 2.9619 - val_loss: 1.4021 - val_accuracy: 0.5139 - val_common_sense_cla: 2.9676\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.9308 - accuracy: 0.6525 - common_sense_cla: 2.9631 - val_loss: 1.3519 - val_accuracy: 0.5285 - val_common_sense_cla: 2.9680\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.8983 - accuracy: 0.6617 - common_sense_cla: 2.9592 - val_loss: 1.4359 - val_accuracy: 0.4993 - val_common_sense_cla: 2.9670\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 0.8608 - accuracy: 0.6755 - common_sense_cla: 2.9624 - val_loss: 1.3697 - val_accuracy: 0.5382 - val_common_sense_cla: 2.9626\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.8318 - accuracy: 0.6884 - common_sense_cla: 2.9626 - val_loss: 1.3871 - val_accuracy: 0.5326 - val_common_sense_cla: 2.9646\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 0.8092 - accuracy: 0.6971 - common_sense_cla: 2.9636 - val_loss: 1.3618 - val_accuracy: 0.5514 - val_common_sense_cla: 2.9672\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.7830 - accuracy: 0.7101 - common_sense_cla: 2.9630 - val_loss: 1.3510 - val_accuracy: 0.5368 - val_common_sense_cla: 2.9653\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 0.7438 - accuracy: 0.7224 - common_sense_cla: 2.9582 - val_loss: 1.4106 - val_accuracy: 0.5312 - val_common_sense_cla: 2.9671\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.7278 - accuracy: 0.7288 - common_sense_cla: 2.9604 - val_loss: 1.3957 - val_accuracy: 0.5562 - val_common_sense_cla: 2.9651\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.6992 - accuracy: 0.7364 - common_sense_cla: 2.9561 - val_loss: 1.3824 - val_accuracy: 0.5681 - val_common_sense_cla: 2.9665\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.6850 - accuracy: 0.7394 - common_sense_cla: 2.9556 - val_loss: 1.3174 - val_accuracy: 0.5771 - val_common_sense_cla: 2.9661\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.6391 - accuracy: 0.7646 - common_sense_cla: 2.9586 - val_loss: 1.3864 - val_accuracy: 0.5681 - val_common_sense_cla: 2.9632\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.6173 - accuracy: 0.7699 - common_sense_cla: 2.9582 - val_loss: 1.4286 - val_accuracy: 0.5729 - val_common_sense_cla: 2.9648\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.5993 - accuracy: 0.7749 - common_sense_cla: 2.9583 - val_loss: 1.4386 - val_accuracy: 0.5583 - val_common_sense_cla: 2.9656\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.5806 - accuracy: 0.7841 - common_sense_cla: 2.9542 - val_loss: 1.4354 - val_accuracy: 0.5819 - val_common_sense_cla: 2.9652\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.5625 - accuracy: 0.7897 - common_sense_cla: 2.9567 - val_loss: 1.4061 - val_accuracy: 0.5840 - val_common_sense_cla: 2.9628\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.5416 - accuracy: 0.7983 - common_sense_cla: 2.9559 - val_loss: 1.4764 - val_accuracy: 0.5743 - val_common_sense_cla: 2.9639\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.5270 - accuracy: 0.8032 - common_sense_cla: 2.9538 - val_loss: 1.5675 - val_accuracy: 0.5674 - val_common_sense_cla: 2.9644\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.5042 - accuracy: 0.8077 - common_sense_cla: 2.9546 - val_loss: 1.4695 - val_accuracy: 0.5757 - val_common_sense_cla: 2.9647\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.4845 - accuracy: 0.8208 - common_sense_cla: 2.9521 - val_loss: 1.6090 - val_accuracy: 0.5576 - val_common_sense_cla: 2.9639\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.4785 - accuracy: 0.8242 - common_sense_cla: 2.9600 - val_loss: 1.5709 - val_accuracy: 0.5750 - val_common_sense_cla: 2.9635\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.4648 - accuracy: 0.8265 - common_sense_cla: 2.9481 - val_loss: 1.5529 - val_accuracy: 0.5694 - val_common_sense_cla: 2.9653\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.4558 - accuracy: 0.8313 - common_sense_cla: 2.9536 - val_loss: 1.6109 - val_accuracy: 0.5813 - val_common_sense_cla: 2.9598\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.4275 - accuracy: 0.8450 - common_sense_cla: 2.9568 - val_loss: 1.6310 - val_accuracy: 0.5813 - val_common_sense_cla: 2.9638\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.4104 - accuracy: 0.8459 - common_sense_cla: 2.9582 - val_loss: 1.5407 - val_accuracy: 0.6069 - val_common_sense_cla: 2.9641\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.4061 - accuracy: 0.8533 - common_sense_cla: 2.9539 - val_loss: 1.6998 - val_accuracy: 0.5826 - val_common_sense_cla: 2.9654\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.3869 - accuracy: 0.8605 - common_sense_cla: 2.9550 - val_loss: 1.7339 - val_accuracy: 0.5472 - val_common_sense_cla: 2.9652\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 5s 27ms/step - loss: 0.3880 - accuracy: 0.8588 - common_sense_cla: 2.9585 - val_loss: 1.7023 - val_accuracy: 0.5847 - val_common_sense_cla: 2.9646\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 0.3961 - accuracy: 0.8511 - common_sense_cla: 2.9555 - val_loss: 1.7029 - val_accuracy: 0.6062 - val_common_sense_cla: 2.9641\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.3596 - accuracy: 0.8708 - common_sense_cla: 2.9570 - val_loss: 1.7200 - val_accuracy: 0.5764 - val_common_sense_cla: 2.9628\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 6s 28ms/step - loss: 0.3285 - accuracy: 0.8822 - common_sense_cla: 2.9505 - val_loss: 1.7490 - val_accuracy: 0.5910 - val_common_sense_cla: 2.9642\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.3268 - accuracy: 0.8840 - common_sense_cla: 2.9548 - val_loss: 1.8536 - val_accuracy: 0.5708 - val_common_sense_cla: 2.9633\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 6s 27ms/step - loss: 0.3504 - accuracy: 0.8724 - common_sense_cla: 2.9566 - val_loss: 1.8305 - val_accuracy: 0.5819 - val_common_sense_cla: 2.9640\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 5s 26ms/step - loss: 0.3056 - accuracy: 0.8924 - common_sense_cla: 2.9588 - val_loss: 1.8613 - val_accuracy: 0.5986 - val_common_sense_cla: 2.9612\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.3047 - accuracy: 0.8910 - common_sense_cla: 2.9596 - val_loss: 1.9224 - val_accuracy: 0.5799 - val_common_sense_cla: 2.9647\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.3063 - accuracy: 0.8916 - common_sense_cla: 2.9535 - val_loss: 2.0369 - val_accuracy: 0.5736 - val_common_sense_cla: 2.9610\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 5s 27ms/step - loss: 0.2967 - accuracy: 0.8949 - common_sense_cla: 2.9534 - val_loss: 2.0721 - val_accuracy: 0.5785 - val_common_sense_cla: 2.9623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f95c125548>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.fit(X,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a347ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40a74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705c654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c19dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5a31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb17817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6f2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db467e66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e799b788",
   "metadata": {},
   "source": [
    "## Task 3: Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce0ed8",
   "metadata": {},
   "source": [
    "In this task, we use MNIST dataset to train the generative models and obtain new figures. We directly call the Tensorflow API to download the dataset. However, the original dataset is also available on this website: <https://deepai.org/dataset/mnist>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f73dea-fe2c-449b-ad2d-ab390c1e07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Conv2DTranspose, Reshape, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc67c38-8af6-4fbf-9d25-bfe3316fd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocessing\n",
    "X = np.concatenate((x_train, x_test), axis=0)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(X)\n",
    "X = X.astype(\"float32\") / 255.\n",
    "X = X.reshape(X.shape + (1,))\n",
    "print(f\"The shape of dataset: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe542e-7c75-4853-aac1-8a17a1f1f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use modified grid_plot functions in the tutorial           \n",
    "def grid_plot(images, epoch=\"\", name=\"\", n=3, save=False, scale=False):\n",
    "    if scale:\n",
    "        images = (images + 1) / 2.0\n",
    "    for index in range(n * n):\n",
    "        plt.subplot(n, n, 1 + index)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[index])\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle(name + '  '+ str(epoch), fontsize=14)\n",
    "    if save:\n",
    "        filename = 'results/generated_plot_e%03d_f.png' % (epoch+1)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8536d-b0f7-4d8b-8caf-ce7b7839590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grid_plot(X[np.random.randint(0, 1000, 9)], name=\"MNIST dataset (28 X 28 X 1)\", n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87954b3-74ed-4ea5-ab1a-68b3f311fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use modified build_conv_net and build_deconv_net functions in the tutorial\n",
    "def build_conv_net(in_shape, out_shape, out_activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a basic convolutional network\n",
    "    \"\"\"\n",
    "    in_put = Input(shape=in_shape)\n",
    "    \n",
    "    x = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu')(in_put)\n",
    "    x = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu', strides=(2,2))(x)\n",
    "    x = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(out_shape, activation=out_activation)(x)\n",
    "    model = tf.keras.Model(in_put, x)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_deconv_net(latent_dim, activation_out='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a deconvolutional network for decoding/upscaling latent vectors\n",
    "\n",
    "    When building the deconvolutional architecture, usually it is best to use the same layer sizes that \n",
    "    were used in the downsampling network and the Conv2DTranspose layers are used instead of Conv2D layers. \n",
    "    Using identical layers and hyperparameters ensures that the dimensionality of our output matches the\n",
    "    shape of our input images. \n",
    "    \"\"\"\n",
    "    in_put = Input(shape=latent_dim)\n",
    "    x = Dense(14 * 14 * 64)(in_put)\n",
    "    x = Reshape((14, 14, 64))(x) # This matches the output size of the downsampling architecture\n",
    "    x = Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "    x = Conv2D(filters=1, kernel_size=3, activation=activation_out, padding='same')(x)\n",
    "    model = tf.keras.Model(in_put, x)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f2366",
   "metadata": {},
   "source": [
    "### Convolutional Autoencoder (CAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1be4c8-6459-449c-b197-9d6eee8d35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAE\n",
    "# Use the build_convolutional_autoencoder function in the tutorial\n",
    "def build_convolutional_autoencoder(data_shape, latent_dim):\n",
    "    encoder = build_conv_net(in_shape=data_shape, out_shape=latent_dim)\n",
    "    decoder = build_deconv_net(latent_dim, activation_out='sigmoid')\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return autoencoder\n",
    "\n",
    "# Defining the model dimensions and building it\n",
    "image_size = X.shape[1:]\n",
    "latent_dim = 32\n",
    "num_filters = 64\n",
    "cae = build_convolutional_autoencoder(image_size, latent_dim)\n",
    "\n",
    "for epoch in range(0, 11):\n",
    "    cae.fit(x=X, y=X, epochs=1, batch_size=64, verbose=0)\n",
    "    print('\\nEpoch: ', epoch)\n",
    "    samples = X[:9]\n",
    "    reconstructed = cae.predict(samples, verbose=0)\n",
    "    grid_plot(samples, epoch, name='Original', n=3, save=False)\n",
    "    grid_plot(reconstructed, epoch, name='Reconstructed', n=3, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031b7b6",
   "metadata": {},
   "source": [
    "### Variational Autoencoders (VAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97220a0e-6eaa-42fb-b193-8ddf41e54e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Sampling class and the build_vae function in the tutorial\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer for the variational autoencoder\n",
    "    It takes two vectors as input - one for means and other for variances of the latent variables described by a multimodal gaussian\n",
    "    Its output is a latent vector randomly sampled from this distribution\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_var) * epsilon\n",
    "\n",
    "    \n",
    "def build_vae(data_shape, latent_dim, filters=128):\n",
    "\n",
    "    # Building the encoder - starts with a simple downsampling convolutional network  \n",
    "    encoder = build_conv_net(data_shape, latent_dim*2)\n",
    "    \n",
    "    # Adding special sampling layer that uses the reparametrization trick \n",
    "    z_mean = Dense(latent_dim)(encoder.output)\n",
    "    z_var = Dense(latent_dim)(encoder.output)\n",
    "    z = Sampling()([z_mean, z_var])\n",
    "    \n",
    "    # Connecting the two encoder parts\n",
    "    encoder = tf.keras.Model(inputs=encoder.input, outputs=z)\n",
    "\n",
    "    # Defining the decoder which is a regular upsampling deconvolutional network\n",
    "    decoder = build_deconv_net(latent_dim, activation_out='sigmoid')\n",
    "    vae = tf.keras.Model(inputs=encoder.input, outputs=decoder(z))\n",
    "    \n",
    "    # Adding the special loss term\n",
    "    kl_loss = -0.5 * tf.reduce_sum(z_var - tf.square(z_mean) - tf.exp(z_var) + 1)\n",
    "    vae.add_loss(kl_loss/tf.cast(tf.keras.backend.prod(data_shape), tf.float32))\n",
    "\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy')\n",
    "\n",
    "    return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf4331-e975-42bb-8c0c-fff00d26daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE model\n",
    "latent_dim = 32\n",
    "encoder, decoder, vae = build_vae(X.shape[1:], latent_dim)\n",
    "\n",
    "# Generate random vectors that we will use to sample our latent space\n",
    "for epoch in range(0, 20):\n",
    "    vae.fit(x=X, y=X, epochs=1, batch_size=16, verbose=1)\n",
    "    latent_vectors = np.random.randn(9, latent_dim)\n",
    "    images = decoder(latent_vectors)\n",
    "    grid_plot(images, epoch, name='VAE generated images (randomly sampled from the latent space)', n=3, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bcad48-d2ae-42a2-8860-2ce28bb8915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "for i in range(-2, 3):\n",
    "    np.random.seed(42)\n",
    "    latent_vectors = np.tile(np.random.randn(latent_dim), (7, 1))\n",
    "    latent_vectors[:, 9] = np.linspace(-5, 5, num=7)\n",
    "    latent_vectors[:, 26] = i * np.ones(7)\n",
    "\n",
    "    images = decoder(latent_vectors)\n",
    "    plt.figure(figsize = (15, 1.5))\n",
    "    for index in range(7):\n",
    "        plt.subplot(1, 7, index + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[index], aspect='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570e5ac",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a1a70-11b7-4fc3-a2fa-8509594f953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use build_gan, run_generator, get_batch, train_gan function in the tutorial\n",
    "def build_gan(data_shape, latent_dim, lr=0.0002, beta_1=0.5):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=beta_1)\n",
    "\n",
    "    # Usually the GAN generator has tanh activation function in the output layer\n",
    "    generator = build_deconv_net(latent_dim, activation_out='tanh')\n",
    "    \n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_conv_net(in_shape=data_shape, out_shape=1) # Single output for binary classification\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    # End-to-end GAN model for training the generator\n",
    "    discriminator.trainable = False\n",
    "    true_fake_prediction = discriminator(generator.output)\n",
    "    GAN = tf.keras.Model(inputs=generator.input, outputs=true_fake_prediction)\n",
    "    GAN = tf.keras.models.Sequential([generator, discriminator])\n",
    "    GAN.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator, generator, GAN\n",
    "\n",
    "\n",
    "def run_generator(generator, n_samples=100):\n",
    "    \"\"\"\n",
    "    Run the generator model and generate n samples of synthetic images using random latent vectors\n",
    "    \"\"\"\n",
    "    latent_dim = generator.layers[0].input_shape[-1]\n",
    "    generator_input = np.random.randn(n_samples, latent_dim[1])\n",
    "\n",
    "    return generator.predict(generator_input)\n",
    "    \n",
    "\n",
    "def get_batch(generator, dataset, batch_size=64):\n",
    "    \"\"\"\n",
    "    Gets a single batch of samples (X) and labels (y) for the training the discriminator.\n",
    "    One half from the real dataset (labeled as 1s), the other created by the generator model (labeled as 0s).\n",
    "    \"\"\"\n",
    "    batch_size //= 2 # Split evenly among fake and real samples\n",
    "\n",
    "    fake_data = run_generator(generator, n_samples=batch_size)\n",
    "    real_data = dataset[np.random.randint(0, dataset.shape[0], batch_size)]\n",
    "\n",
    "    X = np.concatenate([fake_data, real_data], axis=0)\n",
    "    y = np.concatenate([np.zeros([batch_size, 1]), np.ones([batch_size, 1])], axis=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, gan, dataset, latent_dim, n_epochs=20, batch_size=64):\n",
    "\n",
    "    batches_per_epoch = int(dataset.shape[0] / batch_size / 2)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(batches_per_epoch):\n",
    "            \n",
    "            # 1) Train discriminator both on real and synthesized images\n",
    "            X, y = get_batch(generator, dataset, batch_size=batch_size)\n",
    "            discriminator_loss = discriminator.train_on_batch(X, y)\n",
    "\n",
    "            # 2) Train generator (note that now the label of synthetic images is reversed to 1)\n",
    "            X_gan = np.random.randn(batch_size, latent_dim)\n",
    "            y_gan = np.ones([batch_size, 1])\n",
    "            generator_loss = gan.train_on_batch(X_gan, y_gan)\n",
    "            \n",
    "        noise = np.random.randn(16, latent_dim)\n",
    "        images = generator.predict(noise)\n",
    "        grid_plot(images, epoch, name='GAN generated images', n=3, save=False, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f0309-dae4-42cf-8dc1-1692f34dd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model (need around 10 epochs to start seeing some results)\n",
    "latent_dim = 256\n",
    "discriminator, generator, gan = build_gan(X.shape[1:], latent_dim)\n",
    "\n",
    "train_gan(generator, discriminator, gan, X, latent_dim, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110af65a-1881-49c8-a765-b6f960e7ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "np.random.seed(13)\n",
    "latent_vectors = np.tile(np.random.randn(latent_dim), (7, 1))\n",
    "latent_vectors[:, 0] = np.linspace(-30, 30, num=7)\n",
    "\n",
    "images = generator(latent_vectors)\n",
    "plt.figure(figsize = (15, 1.5))\n",
    "for index in range(7):\n",
    "    plt.subplot(1, 7, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index], aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170806b-223f-4ab8-978f-b271171f51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "np.random.seed(13)\n",
    "latent_vectors = np.tile(np.random.randn(latent_dim), (7, 1))\n",
    "latent_vectors[:, 119] = np.linspace(-30, 30, num=7)\n",
    "\n",
    "images = generator(latent_vectors)\n",
    "plt.figure(figsize = (15, 1.5))\n",
    "for index in range(7):\n",
    "    plt.subplot(1, 7, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index], aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce67ad6-604c-428b-bfd7-c17a7e35968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "np.random.seed(13)\n",
    "latent_vectors = np.tile(np.random.randn(latent_dim), (7, 1))\n",
    "latent_vectors[:, 125] = np.linspace(0, 30, num=7)\n",
    "\n",
    "images = generator(latent_vectors)\n",
    "plt.figure(figsize = (15, 1.5))\n",
    "for index in range(7):\n",
    "    plt.subplot(1, 7, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index], aspect='auto')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
